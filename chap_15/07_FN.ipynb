{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fdf4b1f-b632-4988-90c3-6a23b7150258",
   "metadata": {},
   "source": [
    "# Topic Modeling: Financial News\n",
    "\n",
    "### Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50865c97-7f2c-4ab3-84a2-4be57df500f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical Computing\n",
    "import numpy as np\n",
    "\n",
    "# Data Manipulation\n",
    "import pandas as pd\n",
    "\n",
    "# Data Visualziation\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "\n",
    "# Spacy\n",
    "import spacy\n",
    "\n",
    "# StatsModel\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Path & Collection\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict, Counter\n",
    "\n",
    "# IPywidget\n",
    "from ipywidgets import interact, FloatRangeSlider\n",
    "\n",
    "# Scikit-Learn\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Gensim\n",
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.matutils import Sparse2Corpus\n",
    "\n",
    "# PyLDAvis\n",
    "import pyLDAvis\n",
    "from pyLDAvis.gensim_models import prepare\n",
    "\n",
    "# Worlcloud\n",
    "from termcolor import colored\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e8ef683-710c-4d1e-9d7a-65e6dee60709",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "931c14e7-e151-4a19-9620-924d0b7f7777",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "sns.set_style('white')\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "398fdd9e-4093-4f7f-a29b-8ef9f1d21a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56450e87-2074-4411-8634-9bf9f3c0cb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(pd.read_csv('http://ir.dcs.gla.ac.uk/resources/linguistic_utils/stop_words',\n",
    "                             header=None,\n",
    "                             squeeze=True).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e7c6cd-a0c8-4570-bf2d-b5cd47bf45e8",
   "metadata": {},
   "source": [
    "#### Helper Viz Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec73b2e6-b6cc-4409-abb4-ef5976d9f66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_word_list(model, corpus, top=10, save=False):\n",
    "    top_topics = model.top_topics(corpus=corpus, coherence='u_mass', topn=20)\n",
    "    words, probs = [], []\n",
    "    for top_topic, _ in top_topics:\n",
    "        words.append([t[1] for t in top_topic[:top]])\n",
    "        probs.append([t[0] for t in top_topic[:top]])\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(model.num_topics*1.2, 5))\n",
    "    sns.heatmap(pd.DataFrame(probs).T,\n",
    "                annot=pd.DataFrame(words).T,\n",
    "                fmt='',\n",
    "                ax=ax,\n",
    "                cmap='Blues',\n",
    "                cbar=False)\n",
    "    fig.tight_layout()\n",
    "    if save:\n",
    "        fig.savefig(f'fin_news_wordlist_{top}', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc286196-e4c2-4cf7-adef-fc99a4e13faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_coherence(model, corpus, tokens, top=10, cutoff=0.01):\n",
    "    top_topics = model.top_topics(corpus=corpus, coherence='u_mass', topn=20)\n",
    "    word_lists = pd.DataFrame(model.get_topics().T, index=tokens)\n",
    "    order = []\n",
    "    for w, word_list in word_lists.items():\n",
    "        target = set(word_list.nlargest(top).index)\n",
    "        for t, (top_topic, _) in enumerate(top_topics):\n",
    "            if target == set([t[1] for t in top_topic[:top]]):\n",
    "                order.append(t)\n",
    "\n",
    "    fig, axes = plt.subplots(ncols=2, figsize=(15,5))\n",
    "    title = f'# Words with Probability > {cutoff:.2%}'\n",
    "    (word_lists.loc[:, order]>cutoff).sum().reset_index(drop=True).plot.bar(title=title, ax=axes[1]);\n",
    "\n",
    "    umass = model.top_topics(corpus=corpus, coherence='u_mass', topn=20)\n",
    "    pd.Series([c[1] for c in umass]).plot.bar(title='Topic Coherence', ax=axes[0])\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(f'fin_news_coherence_{top}', dpi=300);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ad6c282-150a-4067-be95-d98c1393c2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_top_docs(model, corpus, docs):\n",
    "    doc_topics = model.get_document_topics(corpus)\n",
    "    df = pd.concat([pd.DataFrame(doc_topic, \n",
    "                                 columns=['topicid', 'weight']).assign(doc=i) \n",
    "                    for i, doc_topic in enumerate(doc_topics)])\n",
    "\n",
    "    for topicid, data in df.groupby('topicid'):\n",
    "        print(topicid, docs[int(data.sort_values('weight', ascending=False).iloc[0].doc)])\n",
    "        print(pd.DataFrame(lda.show_topic(topicid=topicid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9578eb-10e9-4723-8702-a1142c91568d",
   "metadata": {},
   "source": [
    "### Loading Financial News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6dc5d627-a07c-4fc7-acda-50e4df9c4fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path('..', 'data', 'us-financial-news')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7d9b556-df9c-440c-b4dd-b95134df3fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "section_titles = ['Press Releases - CNBC',\n",
    "                  'Reuters: Company News',\n",
    "                  'Reuters: World News',\n",
    "                  'Reuters: Business News',\n",
    "                  'Reuters: Financial Services and Real Estate',\n",
    "                  'Top News and Analysis (pro)',\n",
    "                  'Reuters: Top News',\n",
    "                  'The Wall Street Journal &amp; Breaking News, Business, Financial and Economic News, World News and Video',\n",
    "                  'Business &amp; Financial News, U.S &amp; International Breaking News | Reuters',\n",
    "                  'Reuters: Money News',\n",
    "                  'Reuters: Technology News']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b48b00f-d6e3-40cf-bd0f-0be0b147f08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_articles():\n",
    "    articles = []\n",
    "    counter = Counter()\n",
    "    for f in data_path.glob('*/**/*.json'):\n",
    "        article = json.load(f.open())\n",
    "        if article['thread']['section_title'] in set(section_titles):\n",
    "            text = article['text'].lower().split()\n",
    "            counter.update(text)\n",
    "            articles.append(' '.join([t for t in text if t not in stop_words]))\n",
    "    return articles, counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a85a0def-fb63-4b59-8385-34a144d9d097",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles, counter = read_articles()\n",
    "\n",
    "print(f'Done loading {len(articles):,.0f} articles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e743574-51b5-4e73-ab19-207397aa0dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common = (pd.DataFrame(counter.most_common(), columns=['token', 'count'])\n",
    "               .pipe(lambda x: x[~x.token.str.lower().isin(stop_words)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a09e84c-ecca-4eb5-bb03-9fa7937ca4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a482a0-4678-4c20-aa50-b0f1d3415556",
   "metadata": {},
   "source": [
    "### Preprocessing with SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "949b40bf-ebe3-4892-831e-e93f1ba64a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = Path('results', 'financial_news')\n",
    "\n",
    "if not results_path.exists():\n",
    "    results_path.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b7090dd9-10d4-4c6d-b788-9b461639b1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_doc(d):\n",
    "    doc = []\n",
    "    for t in d:\n",
    "        if not any([t.is_stop, t.is_digit, not t.is_alpha, t.is_punct, t.is_space, t.lemma_ == '-PRON-']):        \n",
    "            doc.append(t.lemma_)\n",
    "    return ' '.join(doc)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "caae6776-73bd-4383-9ffc-9bbdb5ff8b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "nlp.max_length = 6000000\n",
    "\n",
    "nlp.disable_pipes('ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0dd769b4-7433-47fb-b6d2-a284ecfb4cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4089e1f3-666b-47bf-9cc5-ad8f1ba3f95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(articles):\n",
    "    iter_articles = (article for article in articles)\n",
    "    clean_articles = []\n",
    "    for i, doc in enumerate(nlp.pipe(iter_articles, \n",
    "                                     batch_size=100, \n",
    "                                     n_threads=8), 1):\n",
    "        if i % 1000 == 0:\n",
    "            print(f'{i / len(articles):.2%}', end=' ', flush=True)\n",
    "        clean_articles.append(clean_doc(doc))\n",
    "    return clean_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "28dd1876-7474-4ac3-b719-65694bbaae09",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_articles = preprocess(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "17ae354c-7fdc-4688-b962-3eaa81642c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_path = results_path / 'clean_text'\n",
    "\n",
    "clean_path.write_text('\\n'.join(clean_articles))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38ace4c-42d9-47ce-b44d-d7aa1daadeaf",
   "metadata": {},
   "source": [
    "### Vectorizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "30e9a557-7d44-4370-93e6-7a65afb8b290",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = clean_path.read_text().split('\\n')\n",
    "\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d16429b-8330-45b6-b97b-636813355665",
   "metadata": {},
   "source": [
    "#### Exploring Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "62beb00d-4420-4496-a52d-49185cb656ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_length, token_count = [], Counter()\n",
    "\n",
    "for i, doc in enumerate(docs, 1):\n",
    "    if i % 1e6 == 0:\n",
    "        print(i, end=' ', flush=True)\n",
    "    d = doc.lower().split()\n",
    "    article_length.append(len(d))\n",
    "    token_count.update(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eec1694a-4728-443b-9493-5b6cbdf20d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2, figsize=(15, 5))\n",
    "\n",
    "(pd.DataFrame(token_count.most_common(), columns=['token', 'count'])\n",
    " .pipe(lambda x: x[~x.token.str.lower().isin(stop_words)])\n",
    " .set_index('token')\n",
    " .squeeze()\n",
    " .iloc[:25]\n",
    " .sort_values()\n",
    " .plot\n",
    " .barh(ax=axes[0], title='Most frequent tokens'))\n",
    "sns.boxenplot(x=pd.Series(article_length), ax=axes[1])\n",
    "axes[1].set_xscale('log')\n",
    "axes[1].set_xlabel('Word Count (log scale)')\n",
    "axes[1].set_title('Article Length Distribution')\n",
    "sns.despine()\n",
    "fig.tight_layout()\n",
    "fig.savefig(results_path / 'fn_explore', dpi=300);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e4b405fd-4d60-4739-8df3-a606bdb92fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(article_length).describe(percentiles=np.arange(.1, 1.0, .1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "601bc38c-1295-48cb-9f38-35784ad07c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [x.lower() for x in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a4629858-501b-4638-9ca4-838f31ea5161",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ad7b58-2454-48a3-962e-69e108102fec",
   "metadata": {},
   "source": [
    "### Setting Vocab Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9794adf4-379c-4b9c-905e-2bb0d4c58993",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_df = .005\n",
    "\n",
    "max_df = .1\n",
    "\n",
    "ngram_range = (1, 1)\n",
    "\n",
    "binary = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8ed5ae93-0195-4262-8326-83f5a600ca2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english',\n",
    "                             min_df=min_df,\n",
    "                             max_df=max_df,\n",
    "                             ngram_range=ngram_range,\n",
    "                             binary=binary)\n",
    "\n",
    "dtm = vectorizer.fit_transform(docs)\n",
    "\n",
    "tokens = vectorizer.get_feature_names()\n",
    "dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b5b17b17-0e59-42ba-986c-2a608a4e49a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Sparse2Corpus(dtm, documents_columns=False)\n",
    "\n",
    "id2word = pd.Series(tokens).to_dict()\n",
    "dictionary = Dictionary.from_corpus(corpus, id2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a702a7fb-e4dc-49fe-8108-9828e2ca23a2",
   "metadata": {},
   "source": [
    "### Training & Evaluating LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2ceb7f2e-3084-428f-8005-4009a24e926b",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename='gensim.log',\n",
    "                    format=\"%(asctime)s:%(levelname)s:%(message)s\",\n",
    "                    level=logging.DEBUG)\n",
    "\n",
    "logging.root.level = logging.DEBUG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5354dfb7-1810-4043-b9c8-d3b225d91802",
   "metadata": {},
   "source": [
    "#### Training Models with 5-25 Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c8c2193a-068a-40d6-a2b3-e2cfb0847988",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = [5, 10, 15, 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bcfc9769-4a3e-4d1a-aa91-72739b39f379",
   "metadata": {},
   "outputs": [],
   "source": [
    "for topics in num_topics:\n",
    "    print(topics)\n",
    "    lda_model = LdaModel(corpus=corpus,\n",
    "                     id2word=id2word,\n",
    "                     num_topics=topics,\n",
    "                     chunksize=len(docs),\n",
    "                     update_every=1,\n",
    "                     alpha='auto',                     \n",
    "                     eta='auto',                       \n",
    "                     decay=0.5,                        \n",
    "                     offset=1.0,\n",
    "                     eval_every=1,\n",
    "                     passes=10,\n",
    "                     iterations=50,\n",
    "                     gamma_threshold=0.001,\n",
    "                     minimum_probability=0.01,         \n",
    "                     minimum_phi_value=0.01,           \n",
    "                     random_state=42)\n",
    "    lda_model.save((results_path / f'model_{topics}').as_posix())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d175c063-f747-40ac-a696-cf51a3b9b7db",
   "metadata": {},
   "source": [
    "#### Evaluating Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bb3851cd-ae45-4259-83d4-0734a01fe7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_lda_model(ntopics, model, corpus=corpus, tokens=tokens):\n",
    "    show_word_list(model=model, corpus=corpus, top=ntopics, save=True)\n",
    "    show_coherence(model=model, corpus=corpus, tokens=tokens, top=ntopics)\n",
    "    vis = prepare(model, corpus, dictionary, mds='tsne')\n",
    "    pyLDAvis.save_html(vis, f'lda_{ntopics}.html')\n",
    "    return 2 ** (-model.log_perplexity(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c8b001ff-7850-40a6-994b-bb3b66335d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_models = {}\n",
    "\n",
    "perplexity ={}\n",
    "\n",
    "for ntopics in num_topics:\n",
    "    print(ntopics)\n",
    "    lda_models[ntopics] = LdaModel.load((results_path / f'model_{ntopics}').as_posix())\n",
    "    perplexity[ntopics] = eval_lda_model(ntopics=ntopics, model=lda_models[ntopics])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9f2521-0b00-4362-a744-57b4cf24f5ec",
   "metadata": {},
   "source": [
    "#### Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c8a5f909-38cb-4bc5-8f0b-63b7862c18bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(perplexity).plot.bar()\n",
    "sns.despine();\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fedfded-b066-4bf8-bbd8-b5dd6aca7a27",
   "metadata": {},
   "source": [
    "### PyLDAVis for 15 Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e5881889-3f4e-4530-b6a2-b32f8ced3d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = prepare(lda_models[15], corpus, dictionary, mds='tsne')\n",
    "\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12dd909-b589-4ce5-bc5b-c906aa3cf06d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
