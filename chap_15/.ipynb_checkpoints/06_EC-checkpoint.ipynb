{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12eed6c4-ddb9-418c-ae58-05e097b38118",
   "metadata": {},
   "source": [
    "# Topic Modeling with Earnings Call Transcripts\n",
    "\n",
    "### Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4a1e58bb-a35f-4664-a107-6ade85367e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical Computing\n",
    "import numpy as np\n",
    "\n",
    "# Data Manipulation\n",
    "import pandas as pd\n",
    "\n",
    "# Data Visualziation\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "\n",
    "# StatsModel\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Path & Collection\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict, Counter\n",
    "\n",
    "# IPywidget\n",
    "from ipywidgets import interact, FloatRangeSlider\n",
    "\n",
    "# Scikit-Learn\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Gensim\n",
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.matutils import Sparse2Corpus\n",
    "\n",
    "# PyLDAvis\n",
    "import pyLDAvis\n",
    "from pyLDAvis.gensim_models import prepare\n",
    "\n",
    "# Worlcloud\n",
    "from termcolor import colored\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bc2689c5-46d6-48fe-b83b-6148192e7581",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "28b4c590-2b54-4b70-aa67-f0ee21061bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "sns.set_style('white')\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d41cf63c-2cb4-4866-b6ee-98cfdbf4283f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1ede4c6-078b-477d-b5c3-cdd691641210",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(pd.read_csv('http://ir.dcs.gla.ac.uk/resources/linguistic_utils/stop_words', \n",
    "                         header=None, \n",
    "                         squeeze=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c297a4f-f3ab-4253-834a-04c2b18a06c9",
   "metadata": {},
   "source": [
    "### Loading Earnings Call Transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b759becc-3e61-45fa-8d99-b6b8af3500e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_DIR = Path().cwd().parent\n",
    "\n",
    "data_path = PROJECT_DIR / 'data' / 'earnings_calls'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fb7dd12-f892-483b-9a7a-ec99c7f2d120",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "\n",
    "for i, transcript in enumerate(data_path.iterdir()):\n",
    "    content = pd.read_csv(transcript / 'content.csv')\n",
    "    documents.extend(content.loc[(content.speaker!='Operator') & (content.content.str.len() > 5), 'content'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aab1f131-27ba-4284-abae-4d932c81a90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db472e9-eb70-4d53-b896-8e29ec68f9c2",
   "metadata": {},
   "source": [
    "### Exploring Data\n",
    "\n",
    "#### Tokens per Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3196e52b-2120-4fe7-8d72-18f5e2c47e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = pd.Series(documents).str.split().str.len()\n",
    "ax = sns.distplot(np.log(word_count), kde=False)\n",
    "ax.set_title('Log word count distribution')\n",
    "sns.despine();\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45f589f4-7b36-4f3b-92b3-9defc35b9f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count.describe(percentiles=np.arange(.1, 1.0, .1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bfd59104-98b3-4745-b856-f947b9a36445",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_count = Counter()\n",
    "\n",
    "for i, doc in enumerate(documents, 1):\n",
    "    if i % 5000 == 0:\n",
    "        print(i, end=' ', flush=True)\n",
    "    token_count.update(doc.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adc7a7c-1b89-488d-99e5-9026ecd5737f",
   "metadata": {},
   "source": [
    "#### Most Frequent Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f98c49b6-590c-4fce-ab97-88b1b01194e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "(pd.DataFrame(token_count.most_common(), columns=['token', 'count'])\n",
    " .pipe(lambda x: x[~x.token.str.lower().isin(stop_words)])\n",
    " .set_index('token')\n",
    " .squeeze()\n",
    " .iloc[:50]\n",
    " .sort_values()\n",
    " .plot\n",
    " .barh(figsize=(8, 10)))\n",
    "sns.despine()\n",
    "plt.tight_layout();\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdd9117-841a-47c1-a612-bc4abb4e7e45",
   "metadata": {},
   "source": [
    "### Preprocessing Transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "91ba1654-124e-42d0-af0b-8b7589674f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_doc(d):\n",
    "    doc = []\n",
    "    for t in d:\n",
    "        if not any([t.is_stop, t.is_digit, not t.is_alpha, t.is_punct, t.is_space, t.lemma_ == '-PRON-']):        \n",
    "            doc.append(t.lemma_)\n",
    "    return ' '.join(doc)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ade760e-1cdc-43fe-9bd3-4ff559b97f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "iter_docs = (doc for doc in documents)\n",
    "\n",
    "clean_docs = []\n",
    "\n",
    "for i, document in enumerate(nlp.pipe(iter_docs, batch_size=100, n_process=8), 1):  \n",
    "    if i % 1000 == 0: \n",
    "        print(f'{i/len(documents):.2%}', end=' ', flush=True)\n",
    "    clean_docs.append(clean_doc(document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99bb071e-8b81-4d85-9e97-d0ae5db0c3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = Path('results', 'earnings_calls')\n",
    "\n",
    "if not results_path.exists():\n",
    "    results_path.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7cd777bf-cd57-4b2b-9228-4ce5099d5cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text = results_path / 'clean_text.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2e99dfac-8a0e-47ae-b12b-69c8b4827500",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text.write_text('\\n'.join(clean_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f254b71b-e39b-43f9-a7af-f64be11edc3d",
   "metadata": {},
   "source": [
    "### Vectorizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aff6fca9-ee79-48ab-9a19-25eb234699f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "\n",
    "for line in clean_text.read_text().split('\\n'):\n",
    "    line = [t for t in line.split() if t not in stop_words]\n",
    "    if len(line) > 10:\n",
    "        docs.append(' '.join(line))\n",
    "\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6291f345-5798-4ebb-802d-b5216fdb21fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_count = Counter()\n",
    "\n",
    "for i, doc in enumerate(docs, 1):\n",
    "    if i % 5000 == 0:\n",
    "        print(i, end=' ', flush=True)\n",
    "    token_count.update(doc.split())\n",
    "\n",
    "token_count = pd.DataFrame(token_count.most_common(), columns=['token', 'count'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e55f4a9-e3b7-4e8e-997c-fb2dc51fb6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = (token_count.set_index('token').squeeze().iloc[:25].sort_values(\n",
    "    ascending=False).plot.bar(figsize=(14, 4), rot=25, title='Most Common Tokens'))\n",
    "\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_xlabel('Token')\n",
    "sns.despine()\n",
    "plt.gcf().tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "310c9c4d-569f-4e76-a1f7-3bf247ad8d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_words = token_count.head(50).token.tolist()\n",
    "\n",
    "binary_vectorizer = CountVectorizer(max_df=1.0,\n",
    "                             min_df=1,\n",
    "                             stop_words=frequent_words,\n",
    "                             max_features=None,\n",
    "                             binary=True)\n",
    "\n",
    "binary_dtm = binary_vectorizer.fit_transform(docs)\n",
    "\n",
    "n_docs, n_tokens = binary_dtm.shape\n",
    "doc_freq = pd.Series(np.array(binary_dtm.sum(axis=0)).squeeze()).div(binary_dtm.shape[0])\n",
    "max_unique_tokens = np.array(binary_dtm.sum(axis=1)).squeeze().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "878d58ec-f481-4741-ac1b-a8c0df1d17a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_range = FloatRangeSlider(value=[0.0, 1.0],\n",
    "                            min=0,\n",
    "                            max=1,\n",
    "                            step=0.0001,\n",
    "                            description='Doc. Freq.',\n",
    "                            disabled=False,\n",
    "                            continuous_update=True,\n",
    "                            orientation='horizontal',\n",
    "                            readout=True,\n",
    "                            readout_format='.1%',\n",
    "                            layout={'width': '800px'})\n",
    "\n",
    "@interact(df_range=df_range)\n",
    "def document_frequency_simulator(df_range):\n",
    "    min_df, max_df = df_range\n",
    "    keep = doc_freq.between(left=min_df, right=max_df)\n",
    "    left = keep.sum()\n",
    "\n",
    "    fig, axes = plt.subplots(ncols=2, figsize=(14, 6))\n",
    "    updated_dtm = binary_dtm.tocsc()[:, np.flatnonzero(keep)]\n",
    "    unique_tokens_per_doc = np.array(updated_dtm.sum(axis=1)).squeeze()\n",
    "    sns.distplot(unique_tokens_per_doc, ax=axes[0], kde=False, norm_hist=False)\n",
    "    axes[0].set_title('Unique Tokens per Doc')\n",
    "    axes[0].set_yscale('log')\n",
    "    axes[0].set_xlabel('# Unique Tokens')\n",
    "    axes[0].set_ylabel('# Documents (log scale)')\n",
    "    axes[0].set_xlim(0, max_unique_tokens)    \n",
    "    axes[0].yaxis.set_major_formatter(ScalarFormatter())\n",
    "\n",
    "    term_freq = pd.Series(np.array(updated_dtm.sum(axis=0)).squeeze())\n",
    "    sns.distplot(term_freq, ax=axes[1], kde=False, norm_hist=False)\n",
    "    axes[1].set_title('Document Frequency')\n",
    "    axes[1].set_ylabel('# Tokens')\n",
    "    axes[1].set_xlabel('# Documents')\n",
    "    axes[1].set_yscale('log')\n",
    "    axes[1].set_xlim(0, n_docs)\n",
    "\n",
    "    title = f'Document/Term Frequency Distribution | # Tokens: {left:,d} ({left/n_tokens:.2%})'\n",
    "    fig.suptitle(title, fontsize=14)\n",
    "    sns.despine()\n",
    "    fig.tight_layout()\n",
    "    fig.subplots_adjust(top=.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b641390-e726-4cfe-85cf-9c63ad21a6cf",
   "metadata": {},
   "source": [
    "### Training & Evaluate LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e36f3279-ae4d-4933-9391-3fdc007a0c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_word_list(model, corpus, top=10, save=False):\n",
    "    top_topics = model.top_topics(corpus=corpus, coherence='u_mass', topn=20)\n",
    "    words, probs = [], []\n",
    "    for top_topic, _ in top_topics:\n",
    "        words.append([t[1] for t in top_topic[:top]])\n",
    "        probs.append([t[0] for t in top_topic[:top]])\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(model.num_topics*1.2, 5))\n",
    "    sns.heatmap(pd.DataFrame(probs).T,\n",
    "                annot=pd.DataFrame(words).T,\n",
    "                fmt='',\n",
    "                ax=ax,\n",
    "                cmap='Blues',\n",
    "                cbar=False)\n",
    "    sns.despine()\n",
    "    fig.tight_layout()\n",
    "    if save:\n",
    "        fig.savefig(results_path / 'earnings_call_wordlist', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c0ee998f-1cf1-4975-b663-316b16ffad81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_coherence(model, corpus, tokens, top=10, cutoff=0.01):\n",
    "    top_topics = model.top_topics(corpus=corpus, coherence='u_mass', topn=20)\n",
    "    word_lists = pd.DataFrame(model.get_topics().T, index=tokens)\n",
    "    order = []\n",
    "    for w, word_list in word_lists.items():\n",
    "        target = set(word_list.nlargest(top).index)\n",
    "        for t, (top_topic, _) in enumerate(top_topics):\n",
    "            if target == set([t[1] for t in top_topic[:top]]):\n",
    "                order.append(t)\n",
    "\n",
    "    fig, axes = plt.subplots(ncols=2, figsize=(15,5))\n",
    "    title = f'# Words with Probability > {cutoff:.2%}'\n",
    "    (word_lists.loc[:, order]>cutoff).sum().reset_index(drop=True).plot.bar(title=title, ax=axes[1]);\n",
    "\n",
    "    umass = model.top_topics(corpus=corpus, coherence='u_mass', topn=20)\n",
    "    pd.Series([c[1] for c in umass]).plot.bar(title='Topic Coherence', ax=axes[0])\n",
    "    sns.despine()\n",
    "    fig.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "71a8cd08-8e34-4fb2-9dbc-645b13f3e66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_top_docs(model, corpus, docs):\n",
    "    doc_topics = model.get_document_topics(corpus)\n",
    "    df = pd.concat([pd.DataFrame(doc_topic, \n",
    "                                 columns=['topicid', 'weight']).assign(doc=i) \n",
    "                    for i, doc_topic in enumerate(doc_topics)])\n",
    "\n",
    "    for topicid, data in df.groupby('topicid'):\n",
    "        print(topicid, docs[int(data.sort_values('weight', ascending=False).iloc[0].doc)])\n",
    "        print(pd.DataFrame(lda.show_topic(topicid=topicid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259bb4b0-295b-4080-942f-a8128da638df",
   "metadata": {},
   "source": [
    "### Vocab Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e8fc7ee2-1a0c-4bac-b14d-b5cbefb6a1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_df = .005\n",
    "\n",
    "max_df=.25\n",
    "\n",
    "ngram_range=(1, 1)\n",
    "\n",
    "binary = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9d0882b7-60a2-4b58-a32e-382c7792bb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words=frequent_words,\n",
    "                             min_df=min_df,\n",
    "                             max_df=max_df,\n",
    "                             ngram_range=ngram_range,\n",
    "                             binary=binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2c9d8335-2146-46f3-b21c-69bb3dd6ddfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm = vectorizer.fit_transform(docs)\n",
    "\n",
    "tokens = vectorizer.get_feature_names()\n",
    "\n",
    "dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3813ff2a-4087-4093-8b21-98177f709588",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Sparse2Corpus(dtm, documents_columns=False)\n",
    "\n",
    "id2word = pd.Series(tokens).to_dict()\n",
    "\n",
    "dictionary = Dictionary.from_corpus(corpus, id2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed2f638-ba35-4b41-8828-314239c0fa20",
   "metadata": {},
   "source": [
    "### Model Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a94dcd3f-477d-4776-bdb3-0de64688314b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics=15\n",
    "chunksize=50000\n",
    "passes=25\n",
    "update_every=None\n",
    "alpha='auto'\n",
    "eta='auto'\n",
    "decay=0.5\n",
    "offset=1.0\n",
    "eval_every=None\n",
    "iterations=50\n",
    "gamma_threshold=0.001\n",
    "minimum_probability=0.01\n",
    "minimum_phi_value=0.01\n",
    "per_word_topics=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "624dc674-644d-4a3c-b853-a80da38b6ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LdaModel(corpus=corpus,\n",
    "               id2word=id2word,\n",
    "               num_topics=num_topics,\n",
    "               chunksize=chunksize,\n",
    "               update_every=update_every,\n",
    "               alpha=alpha,\n",
    "               eta=eta,\n",
    "               decay=decay,\n",
    "               offset=offset,\n",
    "               eval_every=eval_every,\n",
    "               passes=passes,\n",
    "               iterations=iterations,\n",
    "               gamma_threshold=gamma_threshold,\n",
    "               minimum_probability=minimum_probability,\n",
    "               minimum_phi_value=minimum_phi_value,\n",
    "               random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e818dfd0-f715-40c7-b125-52f6194434f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_word_list(model=lda, corpus=corpus, save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecaa22b0-21c6-439c-8a9e-6b9634205a1a",
   "metadata": {},
   "source": [
    "### Topic Coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "126d8759-4932-4b2a-b56a-4dc2cfea137c",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_coherence(model=lda, corpus=corpus, tokens=tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473f348e-8827-404c-be10-e4ac17a6ffd7",
   "metadata": {},
   "source": [
    "### pyLDAVis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "da7cbdaf-8696-489f-b790-35865c84a3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = prepare(lda, corpus, dictionary, mds='tsne')\n",
    "\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "12317862-41b0-481b-9153-da61843bedf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.save_html(vis, (results_path / f'lda_15.html').as_posix())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4362699d-706a-4809-b03c-1312589a5530",
   "metadata": {},
   "source": [
    "#### Show Documents Most Representative of Each Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bcd02f7d-74c0-4fe2-b10c-fe4bb790eb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_top_docs(model=lda, corpus=corpus, docs=docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22997a37-bf54-4a7a-ba9a-d6b8d3be01ea",
   "metadata": {},
   "source": [
    "### Reviewing Experiment Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b765ec44-644e-4b7b-b19c-cae9ae6cfb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.HDFStore(results_path / 'results.h5') as store:\n",
    "    perplexity = store.get('perplexity')\n",
    "    coherence = store.get('coherence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "eea79ddd-6cf3-4099-83ae-3e0c106e907d",
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6c782d80-67f7-4c36-81fd-fadec86f2010",
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9f8f75-ec79-4f0a-9f43-ba1b9232a2e5",
   "metadata": {},
   "source": [
    "### Parameter Settings: `Impact on Perplexity`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e1e6b270-423b-4f39-acd8-b9f487abcf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = perplexity[['min_df', 'max_df', 'binary', 'num_topics','passes']]\n",
    "X = pd.get_dummies(X, columns=X.columns, drop_first=True)\n",
    "\n",
    "ols = sm.OLS(endog=perplexity.perplexity, exog=sm.add_constant(X))\n",
    "model = ols.fit(cov_type='HC0')\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c620883-ecb8-4b04-bf6b-3d7e0c0fe506",
   "metadata": {},
   "source": [
    "### Parameter Settings: `Impact on Coherence`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9d4e4b4b-b2b4-4b94-bf89-f636644a65d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = coherence.drop('coherence', axis=1)\n",
    "X = pd.get_dummies(X, columns=X.columns, drop_first=True)\n",
    "\n",
    "ols = sm.OLS(endog=coherence.coherence, exog=sm.add_constant(X))\n",
    "\n",
    "model = ols.fit(cov_type='HC0')\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7835d6d8-7e62-4f4c-85b2-f385e66e3c85",
   "metadata": {},
   "source": [
    "### Hyperparameter Impact on Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "614660bd-f4ed-42c8-926d-f327b351e2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x='num_topics',\n",
    "            y='perplexity',\n",
    "            data=perplexity,\n",
    "            hue='vocab_size',\n",
    "            col='binary',\n",
    "            row='passes',\n",
    "            kind='strip');\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a7eb1a76-718c-4604-85e5-8bcb2cc2d1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence.num_topics = coherence.num_topics.apply(lambda x: f'model_{int(x):0>2}')\n",
    "\n",
    "perplexity.min_df = perplexity.min_df.apply(lambda x: f'min_df_{int(x):0>3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1cd49d-176a-492a-be81-a01d5c629820",
   "metadata": {},
   "source": [
    "### Hyperparameter Impact on Topic Coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "33d8fc0b-c886-4a42-80c9-16a7942cbce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2, figsize=(16,5))\n",
    "\n",
    "data = coherence.sort_values('num_topics')\n",
    "\n",
    "sns.lineplot(x='topic', y='coherence', hue='num_topics', data=data, lw=2, ax=axes[0])\n",
    "axes[0].set_title('Topic Coherence')\n",
    "sns.stripplot(x='num_topics', y='perplexity', hue='vocab_size', data=perplexity, lw=2, ax=axes[1])\n",
    "axes[1].set_title('Perplexity')\n",
    "sns.despine()\n",
    "fig.tight_layout();\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0f327b-5b78-4acb-8b07-90cbab41a7db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
