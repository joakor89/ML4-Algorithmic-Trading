{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f283735-77e6-419a-b04a-3c938bcb840e",
   "metadata": {},
   "source": [
    "# Reinforcement Learning for Trading - Deep Q-learning & The Stock Market\n",
    "\n",
    "### Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f154a67a-cc8c-43eb-ba56-1e7d17973e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical Computing\n",
    "import numpy as np\n",
    "\n",
    "# Data Manipulation\n",
    "import pandas as pd\n",
    "\n",
    "# Data Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "\n",
    "# Time & Path\n",
    "from time import time\n",
    "from pathlib import Path\n",
    "from random import sample\n",
    "from collections import deque\n",
    "from itertools import product\n",
    "\n",
    "# Warnigns\n",
    "import warnings\n",
    "\n",
    "# TensorFlow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "# Gym\n",
    "import gym\n",
    "from gym.envs.registration import register"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb12b372-b272-4875-91e8-b1594090a7a7",
   "metadata": {},
   "source": [
    "#### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51137a51-39e3-4c1d-af4d-252d3aed44ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ee57cd2-8acc-47b1-b5d3-9e27482a77aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n"
     ]
    }
   ],
   "source": [
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "if gpu_devices:\n",
    "    print('Using GPU')\n",
    "    tf.config.experimental.set_memory_growth(gpu_devices[0], True)\n",
    "else:\n",
    "    print('Using CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a11877a7-bf63-4a36-8d3c-f2df9dd49b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = Path('results', 'trading_bot')\n",
    "\n",
    "if not results_path.exists():\n",
    "    results_path.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcb6e14-30bd-4e6c-a0fb-a64b65ecf2d2",
   "metadata": {},
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01bac3ed-14f3-4b58-bed5-119b5f310ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(t):\n",
    "    m_, s = divmod(t, 60)\n",
    "    h, m = divmod(m_, 60)\n",
    "    return '{:02.0f}:{:02.0f}:{:02.0f}'.format(h, m, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb86228-d406-4fe3-bac3-9330ebe1324f",
   "metadata": {},
   "source": [
    "### Set Gym Environment Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "784d8b7b-8218-422a-ac7c-86d7ddd02c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trading_days = 252"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "86394859-cc6d-4406-9872-143f8ade8d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "register(\n",
    "    id='trading-v0',\n",
    "    entry_point='trading_env:TradingEnvironment',\n",
    "    max_episode_steps=trading_days\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e046dc70-2240-4919-8bcb-72ea85d8fe3d",
   "metadata": {},
   "source": [
    "#### Initialize Trading Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "68cb86c4-eb01-42a7-9f75-b725ceb18c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "trading_cost_bps = 1e-3\n",
    "\n",
    "time_cost_bps = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3a94646d-4e11-47e5-9af7-06fb9c3bdd81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Trading costs: 0.10% | Time costs: 0.01%'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'Trading costs: {trading_cost_bps:.2%} | Time costs: {time_cost_bps:.2%}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "08b5663e-41e3-4db9-a11f-c9986bf9cc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trading_environment = gym.make('trading-v0', \n",
    "                               ticker='AAPL',\n",
    "                               trading_days=trading_days,\n",
    "                               trading_cost_bps=trading_cost_bps,\n",
    "                               time_cost_bps=time_cost_bps)\n",
    "trading_environment.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef8ef5d-c2cf-4e2f-b371-0b59996fdeae",
   "metadata": {},
   "source": [
    "#### Getting Environment Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e97b9874-9533-43c9-9899-a836127fe534",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dim = trading_environment.observation_space.shape[0]\n",
    "\n",
    "num_actions = trading_environment.action_space.n\n",
    "\n",
    "max_episode_steps = trading_environment.spec.max_episode_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2171d180-399d-4446-8e90-ffdf84562db2",
   "metadata": {},
   "source": [
    "### Defining Trading Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "db4a85ad-c925-475c-bd41-39ee232750fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQNAgent:\n",
    "    def __init__(self, state_dim,\n",
    "                 num_actions,\n",
    "                 learning_rate,\n",
    "                 gamma,\n",
    "                 epsilon_start,\n",
    "                 epsilon_end,\n",
    "                 epsilon_decay_steps,\n",
    "                 epsilon_exponential_decay,\n",
    "                 replay_capacity,\n",
    "                 architecture,\n",
    "                 l2_reg,\n",
    "                 tau,\n",
    "                 batch_size):\n",
    "\n",
    "        self.state_dim = state_dim\n",
    "        self.num_actions = num_actions\n",
    "        self.experience = deque([], maxlen=replay_capacity)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.architecture = architecture\n",
    "        self.l2_reg = l2_reg\n",
    "\n",
    "        self.online_network = self.build_model()\n",
    "        self.target_network = self.build_model(trainable=False)\n",
    "        self.update_target()\n",
    "\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_decay_steps = epsilon_decay_steps\n",
    "        self.epsilon_decay = (epsilon_start - epsilon_end) / epsilon_decay_steps\n",
    "        self.epsilon_exponential_decay = epsilon_exponential_decay\n",
    "        self.epsilon_history = []\n",
    "\n",
    "        self.total_steps = self.train_steps = 0\n",
    "        self.episodes = self.episode_length = self.train_episodes = 0\n",
    "        self.steps_per_episode = []\n",
    "        self.episode_reward = 0\n",
    "        self.rewards_history = []\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.tau = tau\n",
    "        self.losses = []\n",
    "        self.idx = tf.range(batch_size)\n",
    "        self.train = True\n",
    "\n",
    "    def build_model(self, trainable=True):\n",
    "        layers = []\n",
    "        n = len(self.architecture)\n",
    "        for i, units in enumerate(self.architecture, 1):\n",
    "            layers.append(Dense(units=units,\n",
    "                                input_dim=self.state_dim if i == 1 else None,\n",
    "                                activation='relu',\n",
    "                                kernel_regularizer=l2(self.l2_reg),\n",
    "                                name=f'Dense_{i}',\n",
    "                                trainable=trainable))\n",
    "        layers.append(Dropout(.1))\n",
    "        layers.append(Dense(units=self.num_actions,\n",
    "                            trainable=trainable,\n",
    "                            name='Output'))\n",
    "        model = Sequential(layers)\n",
    "        model.compile(loss='mean_squared_error',\n",
    "                      optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def update_target(self):\n",
    "        self.target_network.set_weights(self.online_network.get_weights())\n",
    "\n",
    "    def epsilon_greedy_policy(self, state):\n",
    "        self.total_steps += 1\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.choice(self.num_actions)\n",
    "        q = self.online_network.predict(state)\n",
    "        return np.argmax(q, axis=1).squeeze()\n",
    "\n",
    "    def memorize_transition(self, s, a, r, s_prime, not_done):\n",
    "        if not_done:\n",
    "            self.episode_reward += r\n",
    "            self.episode_length += 1\n",
    "        else:\n",
    "            if self.train:\n",
    "                if self.episodes < self.epsilon_decay_steps:\n",
    "                    self.epsilon -= self.epsilon_decay\n",
    "                else:\n",
    "                    self.epsilon *= self.epsilon_exponential_decay\n",
    "\n",
    "            self.episodes += 1\n",
    "            self.rewards_history.append(self.episode_reward)\n",
    "            self.steps_per_episode.append(self.episode_length)\n",
    "            self.episode_reward, self.episode_length = 0, 0\n",
    "\n",
    "        self.experience.append((s, a, r, s_prime, not_done))\n",
    "\n",
    "    def experience_replay(self):\n",
    "        if self.batch_size > len(self.experience):\n",
    "            return\n",
    "        minibatch = map(np.array, zip(*sample(self.experience, self.batch_size)))\n",
    "        states, actions, rewards, next_states, not_done = minibatch\n",
    "\n",
    "        next_q_values = self.online_network.predict_on_batch(next_states)\n",
    "        best_actions = tf.argmax(next_q_values, axis=1)\n",
    "\n",
    "        next_q_values_target = self.target_network.predict_on_batch(next_states)\n",
    "        target_q_values = tf.gather_nd(next_q_values_target,\n",
    "                                       tf.stack((self.idx, tf.cast(best_actions, tf.int32)), axis=1))\n",
    "\n",
    "        targets = rewards + not_done * self.gamma * target_q_values\n",
    "\n",
    "        q_values = self.online_network.predict_on_batch(states)\n",
    "        q_values[[self.idx, actions]] = targets\n",
    "\n",
    "        loss = self.online_network.train_on_batch(x=states, y=q_values)\n",
    "        self.losses.append(loss)\n",
    "\n",
    "        if self.total_steps % self.tau == 0:\n",
    "            self.update_target()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25c7085-545d-49c3-8a17-a2a1860eec96",
   "metadata": {},
   "source": [
    "### Defining Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c6c4df90-396e-43bc-90d2-864a4ea501cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = .99,  \n",
    "\n",
    "tau = 100  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb134f1-1f46-4cc0-a0a2-e6dd42710e80",
   "metadata": {},
   "source": [
    "#### NN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7cd13ae1-c3a9-4d58-93c4-fa7c546da978",
   "metadata": {},
   "outputs": [],
   "source": [
    "architecture = (256, 256)  \n",
    "\n",
    "learning_rate = 0.0001  \n",
    "\n",
    "l2_reg = 1e-6  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ab443b-b8a1-4fa0-bda3-810ec4723975",
   "metadata": {},
   "source": [
    "#### Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "79290242-df9c-4cab-8292-c3fc73268b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_capacity = int(1e6)\n",
    "\n",
    "batch_size = 4096"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80157cd-12c7-4cb6-8f84-0fcbd05ab7f8",
   "metadata": {},
   "source": [
    "#### Greedy Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7eaa0d65-7b41-46f8-b470-bfbfec5b97a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_start = 1.0\n",
    "\n",
    "epsilon_end = .01\n",
    "\n",
    "epsilon_decay_steps = 250\n",
    "\n",
    "epsilon_exponential_decay = .99"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2bc573-8557-423d-9248-f8d404b5ba66",
   "metadata": {},
   "source": [
    "### Creating DDQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b28705a2-1a3e-4b1f-95d7-afbff277abb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "5ca58ba1-b955-4d1f-9da9-acf175904d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddqn = DDQNAgent(state_dim=state_dim,\n",
    "                 num_actions=num_actions,\n",
    "                 learning_rate=learning_rate,\n",
    "                 gamma=gamma,\n",
    "                 epsilon_start=epsilon_start,\n",
    "                 epsilon_end=epsilon_end,\n",
    "                 epsilon_decay_steps=epsilon_decay_steps,\n",
    "                 epsilon_exponential_decay=epsilon_exponential_decay,\n",
    "                 replay_capacity=replay_capacity,\n",
    "                 architecture=architecture,\n",
    "                 l2_reg=l2_reg,\n",
    "                 tau=tau,\n",
    "                 batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "97530cd9-90bf-4e8b-bbb7-803d37ea09b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddqn.online_network.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0901f19-72af-4f5d-a24e-45cc47046410",
   "metadata": {},
   "source": [
    "### Running Experiment\n",
    "\n",
    "#### Setting Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "75b0ecce-76fd-4b4f-93ec-b2bb643117b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_steps = 0\n",
    "\n",
    "max_episodes = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b702f6-957d-4597-97fc-dfe3901c561d",
   "metadata": {},
   "source": [
    "#### Initialize variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "1d56c6b6-45a7-4328-9e17-6940fb494bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_time, navs, market_navs, diffs, episode_eps = [], [], [], [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0935a3d7-1ef1-4a0c-b3b5-30f2e7bf9441",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "eac48650-692c-4129-8dfc-cdc0cb715bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def track_results(episode, nav_ma_100, nav_ma_10,\n",
    "                  market_nav_100, market_nav_10,\n",
    "                  win_ratio, total, epsilon):\n",
    "    time_ma = np.mean([episode_time[-100:]])\n",
    "    T = np.sum(episode_time)\n",
    "    \n",
    "    template = '{:>4d} | {} | Agent: {:>6.1%} ({:>6.1%}) | '\n",
    "    template += 'Market: {:>6.1%} ({:>6.1%}) | '\n",
    "    template += 'Wins: {:>5.1%} | eps: {:>6.3f}'\n",
    "    print(template.format(episode, format_time(total), \n",
    "                          nav_ma_100-1, nav_ma_10-1, \n",
    "                          market_nav_100-1, market_nav_10-1, \n",
    "                          win_ratio, epsilon))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4e7833-cf2b-4de2-8e93-18a753f22aaa",
   "metadata": {},
   "source": [
    "### Training Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "5ea35770-03c5-4404-8d08-ddc963eeb3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "results = []\n",
    "for episode in range(1, max_episodes + 1):\n",
    "    this_state = trading_environment.reset()\n",
    "    for episode_step in range(max_episode_steps):\n",
    "        action = ddqn.epsilon_greedy_policy(this_state.reshape(-1, state_dim))\n",
    "        next_state, reward, done, _ = trading_environment.step(action)\n",
    "    \n",
    "        ddqn.memorize_transition(this_state, \n",
    "                                 action, \n",
    "                                 reward, \n",
    "                                 next_state, \n",
    "                                 0.0 if done else 1.0)\n",
    "        if ddqn.train:\n",
    "            ddqn.experience_replay()\n",
    "        if done:\n",
    "            break\n",
    "        this_state = next_state\n",
    "\n",
    "    # get DataFrame with seqence of actions, returns and nav values\n",
    "    result = trading_environment.env.simulator.result()\n",
    "    \n",
    "    # get results of last step\n",
    "    final = result.iloc[-1]\n",
    "\n",
    "    # apply return (net of cost) of last action to last starting nav \n",
    "    nav = final.nav * (1 + final.strategy_return)\n",
    "    navs.append(nav)\n",
    "\n",
    "    # market nav \n",
    "    market_nav = final.market_nav\n",
    "    market_navs.append(market_nav)\n",
    "\n",
    "    # track difference between agent an market NAV results\n",
    "    diff = nav - market_nav\n",
    "    diffs.append(diff)\n",
    "    \n",
    "    if episode % 10 == 0:\n",
    "        track_results(episode, \n",
    "                      # show mov. average results for 100 (10) periods\n",
    "                      np.mean(navs[-100:]), \n",
    "                      np.mean(navs[-10:]), \n",
    "                      np.mean(market_navs[-100:]), \n",
    "                      np.mean(market_navs[-10:]), \n",
    "                      # share of agent wins, defined as higher ending nav\n",
    "                      np.sum([s > 0 for s in diffs[-100:]])/min(len(diffs), 100), \n",
    "                      time() - start, ddqn.epsilon)\n",
    "    if len(diffs) > 25 and all([r > 0 for r in diffs[-25:]]):\n",
    "        print(result.tail())\n",
    "        break\n",
    "\n",
    "trading_environment.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0a2e52-58d6-4a55-a19d-03c2b2074b4e",
   "metadata": {},
   "source": [
    "#### Storing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "1bf28fe2-8f2b-4a54-821d-0d1df61da2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({'Episode': list(range(1, episode+1)),\n",
    "                        'Agent': navs,\n",
    "                        'Market': market_navs,\n",
    "                        'Difference': diffs}).set_index('Episode')\n",
    "\n",
    "results['Strategy Wins (%)'] = (results.Difference > 0).rolling(100).sum()\n",
    "results.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "3fd36c22-0fb7-4f15-910f-9bf4456a0d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(results_path / 'results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "86a800cf-0d85-4ac2-a80b-c4f061706eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style('white'):\n",
    "    sns.distplot(results.Difference)\n",
    "    sns.despine()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042676df-6d16-44d8-8b70-b0b17012cce0",
   "metadata": {},
   "source": [
    "#### Evaluating Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "b7a61b56-e5b9-4146-8fa6-b308f78d11c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "25273a0f-b4b3-4912-9d87-6599cf1b11de",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2, figsize=(14, 4), sharey=True)\n",
    "\n",
    "df1 = (results[['Agent', 'Market']]\n",
    "       .sub(1)\n",
    "       .rolling(100)\n",
    "       .mean())\n",
    "df1.plot(ax=axes[0],\n",
    "         title='Annual Returns (Moving Average)',\n",
    "         lw=1)\n",
    "\n",
    "df2 = results['Strategy Wins (%)'].div(100).rolling(50).mean()\n",
    "df2.plot(ax=axes[1],\n",
    "         title='Agent Outperformance (%, Moving Average)')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.yaxis.set_major_formatter(\n",
    "        FuncFormatter(lambda y, _: '{:.0%}'.format(y)))\n",
    "    ax.xaxis.set_major_formatter(\n",
    "        FuncFormatter(lambda x, _: '{:,.0f}'.format(x)))\n",
    "axes[1].axhline(.5, ls='--', c='k', lw=1)\n",
    "\n",
    "sns.despine()\n",
    "fig.tight_layout()\n",
    "fig.savefig(results_path / 'performance', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcabf84-ce47-4fe8-a6b2-27e5a5b89c8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:MLAT] *",
   "language": "python",
   "name": "conda-env-MLAT-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
