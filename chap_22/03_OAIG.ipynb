{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c380c72-8425-4b14-bed3-07444f79e8a3",
   "metadata": {},
   "source": [
    "# Double Deep Q-Learning & Open AI Gym: Intro\n",
    "\n",
    "## The Open AI Lunar Lander environment - Deep Q-Learning\n",
    "\n",
    "### Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "41f9b858-bd28-4891-93dd-d8fcec6f8920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical Computing\n",
    "import numpy as np\n",
    "\n",
    "# Data Manipulation\n",
    "import pandas as pd\n",
    "\n",
    "# Data Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from mdptoolbox import mdp\n",
    "from itertools import product\n",
    "\n",
    "# Time & Path\n",
    "import time\n",
    "from pathlib import Path\n",
    "from time import process_time\n",
    "\n",
    "# Warnigns\n",
    "import warnings\n",
    "\n",
    "# TensorFlow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# OpenAI Gym\n",
    "import gym\n",
    "from gym import wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "10b6ec26-571e-4428-8f2b-2d069d450276",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_style('whitegrid', {'axes.grid' : False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "98845dc8-047c-4726-8940-8048dbdc629e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n"
     ]
    }
   ],
   "source": [
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "if gpu_devices:\n",
    "    print('Using GPU')\n",
    "    tf.config.experimental.set_memory_growth(gpu_devices[0], True)\n",
    "else:\n",
    "    print('Using CPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6d3013-a800-4e35-9bf4-4052ba5f2e6d",
   "metadata": {},
   "source": [
    "#### Display Helper Functions Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ea9e0063-08a8-4a60-9f95-d92501c9b14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(t):\n",
    "    m_, s = divmod(t, 60)\n",
    "    h, m = divmod(m_, 60)\n",
    "    return '{:02.0f}:{:02.0f}:{:02.0f}'.format(h, m, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbb37a4-09d8-4c36-a323-55c5c9f61303",
   "metadata": {},
   "source": [
    "#### Enable Virtual Display to Run from Docker Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "37163343-37a3-448f-bc3e-57daa414631b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyvirtualdisplay import Display\n",
    "# virtual_display = Display(visible=0, size=(1400, 900))\n",
    "# virtual_display.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6e2a8f-c83b-4115-a20b-072b4c320123",
   "metadata": {},
   "source": [
    "### Defining DDQN Agent\n",
    "\n",
    "#### Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "26a80eda-7d32-403e-9768-c0d2874037e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    def __init__(self, capacity, state_dims):\n",
    "        self.capacity = capacity\n",
    "        self.idx = 0\n",
    "\n",
    "        self.state_memory = np.zeros(shape=(capacity, state_dims), \n",
    "                                     dtype=np.float32)\n",
    "        self.new_state_memory = np.zeros_like(self.state_memory)\n",
    "\n",
    "        self.action_memory = np.zeros(capacity, dtype=np.int32)\n",
    "        self.reward_memory = np.zeros_like(self.action_memory)\n",
    "        self.done = np.zeros_like(self.action_memory)\n",
    "\n",
    "    def store(self, state, action, reward, next_state, done):\n",
    "        self.state_memory[self.idx, :] = state\n",
    "        self.new_state_memory[self.idx, :] = next_state\n",
    "        self.reward_memory[self.idx] = reward\n",
    "        self.action_memory[self.idx] = action\n",
    "        self.done[self.idx] = 1 - int(done)\n",
    "        self.idx += 1\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = np.random.choice(self.idx, batch_size, replace=False)\n",
    "\n",
    "        states = self.state_memory[batch]\n",
    "        next_states = self.new_state_memory[batch]\n",
    "        rewards = self.reward_memory[batch]\n",
    "        actions = self.action_memory[batch]\n",
    "        done = self.done[batch]\n",
    "        return states, actions, rewards, next_states, done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7712068-43d2-4b63-b7a6-311b1c079c6a",
   "metadata": {},
   "source": [
    "#### Agent Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5d90b22a-4ee1-4283-8328-d31f0dc7aa55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQNAgent:\n",
    "    def __init__(self,\n",
    "                 state_dim,\n",
    "                 num_actions,\n",
    "                 gamma,\n",
    "                 epsilon_start,\n",
    "                 epsilon_end,\n",
    "                 epsilon_decay_steps,\n",
    "                 epsilon_exponential_decay,\n",
    "                 learning_rate,\n",
    "                 architecture,\n",
    "                 l2_reg,\n",
    "                 replay_capacity,\n",
    "                 tau,\n",
    "                 batch_size,\n",
    "                 results_dir,\n",
    "                 log_every=10):\n",
    "\n",
    "        self.state_dim = state_dim\n",
    "        self.num_actions = num_actions\n",
    "\n",
    "        self.architecture = architecture\n",
    "        self.l2_reg = l2_reg\n",
    "        self.learning_rate = learning_rate\n",
    "        self.experience = Memory(replay_capacity,\n",
    "                                 state_dim)\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.batch_size = batch_size\n",
    "        self.idx = np.arange(batch_size, dtype=np.int32)\n",
    "\n",
    "        self.online_network = self.build_model()\n",
    "        self.target_network = self.build_model(trainable=False)\n",
    "        self.optimizer = Adam(lr=learning_rate)\n",
    "        self.update_target()\n",
    "\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_decay_steps = epsilon_decay_steps\n",
    "        self.epsilon_decay = (epsilon_start - epsilon_end) / epsilon_decay_steps\n",
    "        self.epsilon_exponential_decay = epsilon_exponential_decay\n",
    "        self.epsilon_history = []\n",
    "\n",
    "        self.total_steps = self.train_steps = 0\n",
    "        self.episodes = self.episode_length = self.train_episodes = 0\n",
    "        self.steps_per_episode = []\n",
    "        self.episode_reward = 0\n",
    "        self.rewards_history = []\n",
    "\n",
    "        self.results_dir = results_dir\n",
    "        self.experiment = experiment\n",
    "        self.log_every = log_every\n",
    "        \n",
    "        self.summary_writer = (tf.summary\n",
    "                               .create_file_writer(results_dir.as_posix()))\n",
    "        self.start = time()\n",
    "        self.train = True\n",
    "\n",
    "    def build_model(self, trainable=True):\n",
    "        layers = []\n",
    "        for i, units in enumerate(self.architecture, 1):\n",
    "            layers.append(Dense(units=units,\n",
    "                                input_dim=self.state_dim if i == 1 else None,\n",
    "                                activation='relu',\n",
    "                                kernel_regularizer=l2(self.l2_reg),\n",
    "                                trainable=trainable))\n",
    "        layers.append(Dense(units=self.num_actions,\n",
    "                            trainable=trainable))\n",
    "        return Sequential(layers)\n",
    "\n",
    "    def update_target(self):\n",
    "        self.target_network.set_weights(self.online_network.get_weights())\n",
    "\n",
    "    # @tf.function\n",
    "    def epsilon_greedy_policy(self, state):\n",
    "        self.total_steps += 1\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.choice(self.num_actions)\n",
    "        q = self.online_network.predict(state)\n",
    "        return np.argmax(q, axis=1).squeeze()\n",
    "\n",
    "    # @tf.function\n",
    "    def decay_epsilon(self):\n",
    "        if self.train:\n",
    "            if self.episodes < self.epsilon_decay_steps:\n",
    "                self.epsilon -= self.epsilon_decay\n",
    "            else:\n",
    "                self.epsilon *= self.epsilon_exponential_decay\n",
    "\n",
    "    def log_progress(self):\n",
    "        self.rewards_history.append(self.episode_reward)\n",
    "        self.steps_per_episode.append(self.episode_length)\n",
    "\n",
    "        avg_steps_100 = np.mean(self.steps_per_episode[-100:])\n",
    "        avg_steps_10 = np.mean(self.steps_per_episode[-10:])\n",
    "        max_steps_10 = np.max(self.steps_per_episode[-10:])\n",
    "        avg_rewards_100 = np.mean(self.rewards_history[-100:])\n",
    "        avg_rewards_10 = np.mean(self.rewards_history[-10:])\n",
    "        max_rewards_10 = np.max(self.rewards_history[-10:])\n",
    "\n",
    "        with self.summary_writer.as_default():\n",
    "            tf.summary.scalar('Episode Reward', self.episode_reward, step=self.episodes)\n",
    "            tf.summary.scalar('Episode Rewards (MA 100)', avg_rewards_100, step=self.episodes)\n",
    "            tf.summary.scalar('Episode Steps', self.episode_length, step=self.episodes)\n",
    "            tf.summary.scalar('Epsilon', self.epsilon, step=self.episodes)\n",
    "\n",
    "        if self.episodes % self.log_every == 0:\n",
    "            template = '{:03} | {} | Rewards {:4.0f} {:4.0f} {:4.0f} | ' \\\n",
    "                       'Steps: {:4.0f} {:4.0f} {:4.0f} | Epsilon: {:.4f}'\n",
    "            print(template.format(self.episodes, format_time(time() - self.start),\n",
    "                                  avg_rewards_100, avg_rewards_10, max_rewards_10,\n",
    "                                  avg_steps_100, avg_steps_10, max_steps_10,\n",
    "                                  self.epsilon))\n",
    "\n",
    "    def memorize_transition(self, s, a, r, s_prime, done):\n",
    "        self.experience.store(s, a, r, s_prime, done)\n",
    "        self.episode_reward += r\n",
    "        self.episode_length += 1\n",
    "\n",
    "        if done:\n",
    "            self.epsilon_history.append(self.epsilon)\n",
    "            self.decay_epsilon()\n",
    "            self.episodes += 1\n",
    "            self.log_progress()\n",
    "            self.episode_reward = 0\n",
    "            self.episode_length = 0\n",
    "\n",
    "    def experience_replay(self):\n",
    "        # not enough experience yet\n",
    "        if self.batch_size > self.experience.idx:\n",
    "            return\n",
    "\n",
    "        # sample minibatch\n",
    "        states, actions, rewards, next_states, done = self.experience.sample(self.batch_size)\n",
    "\n",
    "        # select best next action (online)\n",
    "        next_action = tf.argmax(self.online_network.predict(next_states, self.batch_size), axis=1, name='next_action')\n",
    "        # predict next q values (target)\n",
    "        next_q_values = self.target_network.predict(next_states, self.batch_size)\n",
    "        # get q values for best next action\n",
    "        target_q = (tf.math.reduce_sum(next_q_values *\n",
    "                                       tf.one_hot(next_action,\n",
    "                                                  self.num_actions),\n",
    "                                       axis=1, name='target_q'))\n",
    "        # compute td target\n",
    "        td_target = rewards + done * self.gamma * target_q\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            q_values = self.online_network(states)\n",
    "            q_values = tf.math.reduce_sum(q_values * tf.one_hot(actions, self.num_actions), axis=1, name='q_values')\n",
    "            loss = tf.math.reduce_mean(tf.square(td_target - q_values))\n",
    "\n",
    "        # run back propagation\n",
    "        variables = self.online_network.trainable_variables\n",
    "        gradients = tape.gradient(loss, variables)\n",
    "\n",
    "        self.optimizer.apply_gradients(zip(gradients, variables))\n",
    "        with self.summary_writer.as_default():\n",
    "            tf.summary.scalar('Loss', loss, step=self.train_steps)\n",
    "        self.train_steps += 1\n",
    "\n",
    "        if self.total_steps % self.tau == 0:\n",
    "            self.update_target()\n",
    "\n",
    "    def store_results(self):\n",
    "        result = pd.DataFrame({'Rewards': self.rewards_history,\n",
    "                               'Steps'  : self.steps_per_episode,\n",
    "                               'Epsilon': self.epsilon_history},\n",
    "                              index=list(range(1, len(self.rewards_history) + 1)))\n",
    "\n",
    "        result.to_csv(self.results_dir / 'results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0244a433-0a69-4529-ab1c-9d3dc67132f5",
   "metadata": {},
   "source": [
    "### Running Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b7b605ec-123b-4bb1-bb3d-3409d9dd94af",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "40528cca-4af8-49fd-977a-51b2775b87e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = Path('results', 'lunar_lander', 'experiment_{}'.format(experiment))\n",
    "\n",
    "if not results_dir.exists():\n",
    "    results_dir.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac42729-76dd-4009-9088-6714412b347c",
   "metadata": {},
   "source": [
    "#### Set  OpenAI Gym Lunar Lander Environment Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d2250d7f-49e9-4221-b9d2-5ffef13be59c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.00229702,  1.4181306 ,  0.23264714,  0.32046658, -0.00265488,\n",
       "        -0.05269808,  0.        ,  0.        ], dtype=float32),\n",
       " {})"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "\n",
    "state_dim = env.observation_space.shape[0]  # number of dimensions in state\n",
    "num_actions = env.action_space.n  # number of actions\n",
    "max_episode_steps = env.spec.max_episode_steps  # max number of steps per episode\n",
    "\n",
    "env.reset(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "0ec3eb99-69cd-49e7-afc8-9279453d5de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor_path = results_dir / 'monitor'\n",
    "video_freq = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "5639e17d-d94b-4fbd-aa87-2b0bde2f136b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = wrappers.Monitor(env,\n",
    "                       directory=monitor_path.as_posix(),\n",
    "                       video_callable=lambda count: count % video_freq == 0,\n",
    "                      force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424bb483-46c3-459c-8fe5-3adc4636ea9b",
   "metadata": {},
   "source": [
    "### Defining Hyperparameters\n",
    "\n",
    "#### Discount Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "c256be9b-f98f-48b8-aa40-fd3e6f4f0b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = .99"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b36d40-f169-46c8-88fb-85fe553fd26e",
   "metadata": {},
   "source": [
    "#### Q-Network Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "93ebce48-7538-4694-b7f1-9056e8cd5f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "18dcb493-bdfb-49df-a031-6d6b5fd583bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "architecture = (256, 256)  \n",
    "\n",
    "l2_reg = 1e-6  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6815c8da-7488-4039-8f5d-090184e66083",
   "metadata": {},
   "source": [
    "#### Replay Buffer Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "49c247c8-5596-4a89-9a7c-c644a44c3403",
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = 100  \n",
    "\n",
    "replay_capacity = int(1e6)\n",
    "\n",
    "batch_size = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93c099b-a7df-427c-9c16-40e687fcb7a9",
   "metadata": {},
   "source": [
    "#### ε-greedy Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "45b65c89-334a-4da8-8338-36ec3bf7dda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_start = 1.0\n",
    "\n",
    "epsilon_end = 0.01\n",
    "\n",
    "epsilon_decay_steps = 100\n",
    "\n",
    "epsilon_exponential_decay = .99"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fb3fc7-958e-464a-8ea0-a23cb68ea130",
   "metadata": {},
   "source": [
    "### Instantiate DDQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "4628fe0c-e0bb-4c85-aee5-24a7ef450bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DDQNAgent(state_dim=state_dim,\n",
    "                  num_actions=num_actions,\n",
    "                  learning_rate=learning_rate,\n",
    "                  gamma=gamma,\n",
    "                  epsilon_start=epsilon_start,\n",
    "                  epsilon_end=epsilon_end,\n",
    "                  epsilon_decay_steps=epsilon_decay_steps,\n",
    "                  epsilon_exponential_decay=epsilon_exponential_decay,\n",
    "                  replay_capacity=replay_capacity,\n",
    "                  architecture=architecture,\n",
    "                  l2_reg=l2_reg,\n",
    "                  tau=tau,\n",
    "                  batch_size=batch_size,\n",
    "                  results_dir=results_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203b07fc-51b8-4259-95a2-973a1fc3a3c2",
   "metadata": {},
   "source": [
    "### Training & Testing Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "4d12e076-0351-427a-b670-6707034809f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "3293145e-bf45-4262-93c6-95d5e10643ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_episodes = 2500\n",
    "\n",
    "test_episodes = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "db948b07-33a7-4e53-9ab3-2352fad6585b",
   "metadata": {},
   "outputs": [],
   "source": [
    "while agent.episodes < max_episodes:\n",
    "    this_state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.epsilon_greedy_policy(this_state.reshape(-1, state_dim))\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        agent.memorize_transition(this_state, action, reward, next_state, done)\n",
    "        agent.experience_replay()\n",
    "        this_state = next_state\n",
    "    if np.mean(agent.rewards_history[-100:]) > 200:\n",
    "        break\n",
    "\n",
    "agent.store_results()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95d4ff5-3448-4839-8fda-17ce4430835e",
   "metadata": {},
   "source": [
    "### Evaluating Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "c3586527-605c-460d-af8c-c146a327e91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv(results_dir / 'results.csv')\n",
    "\n",
    "results['MA100'] = results.rolling(window=100, min_periods=25).Rewards.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "a497d545-4cab-4c9e-8844-cbed539358b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2, figsize=(16, 4), sharex=True)\n",
    "results[['Rewards', 'MA100']].plot(ax=axes[0])\n",
    "axes[0].set_ylabel('Rewards')\n",
    "axes[0].set_xlabel('Episodes')\n",
    "axes[0].axhline(200, c='k', ls='--', lw=1)\n",
    "results[['Steps', 'Epsilon']].plot(secondary_y='Epsilon', ax=axes[1]);\n",
    "axes[1].set_xlabel('Episodes')\n",
    "fig.suptitle('Double Deep Q-Network Agent | Lunar Lander', fontsize=16)\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=.9)\n",
    "fig.savefig(results_dir / 'trading_agent_2ed', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a103145-62e2-4f3c-8355-9ff6b3b1159d",
   "metadata": {},
   "source": [
    "### Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "73e0a9da-fe95-486d-9a2c-096cdba0004f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "2d7f8a74-56ed-4481-a3c1-fc23c41ce298",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir results/lunar_lander/experiment_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44312705-651b-4687-92ef-89007b0b05a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:MLAT] *",
   "language": "python",
   "name": "conda-env-MLAT-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
