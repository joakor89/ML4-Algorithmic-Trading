{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b372cc1-9bb9-495b-8a0c-d9b13b8ed65d",
   "metadata": {},
   "source": [
    "# GBM Hyperparameter Tuning with sklearn\n",
    "\n",
    "### Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dabe43bb-a73c-4e5e-b861-c81ec1c29e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/MLAT/lib/python3.12/site-packages/sklearn/experimental/enable_hist_gradient_boosting.py:19: UserWarning: Since version 1.0, it is not needed to import enable_hist_gradient_boosting anymore. HistGradientBoostingClassifier and HistGradientBoostingRegressor are now stable and can be normally imported from sklearn.ensemble.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Numerical Computing\n",
    "import numpy as np\n",
    "\n",
    "# Data Manipulation\n",
    "import pandas as pd\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "\n",
    "# Scikit-Learn\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import GradientBoostingClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from itertools import product\n",
    "\n",
    "# Time & Joblib\n",
    "import joblib\n",
    "from time import time\n",
    "\n",
    "# Path\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "836e6280-9c9b-48fb-bbf1-d624aca52b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e7e829-619e-4de8-b97d-86072df0c1c8",
   "metadata": {},
   "source": [
    "### Create One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe90e955-4b81-4eb3-a458-533334a59a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_hot_data(df, cols=('year', 'month', 'age', 'msize')):\n",
    "    cols = list(cols)\n",
    "    df = pd.get_dummies(df,\n",
    "                        columns=cols + ['sector'],\n",
    "                        prefix=cols + [''],\n",
    "                        prefix_sep=['_'] * len(cols) + [''])\n",
    "    return df.rename(columns={c: c.replace('.0', '').replace(' ', '_').lower() for c in df.columns})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2274492e-e81d-46b7-9a9f-dec8f831fe72",
   "metadata": {},
   "source": [
    "### Creating Hold-Out Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdde2661-a906-453a-a217-d8734fd6ed15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_holdout_set(target, features, period=6):\n",
    "    idx = pd.IndexSlice\n",
    "    label = target.name\n",
    "    dates = np.sort(target.index.get_level_values('date').unique())\n",
    "    cv_start, cv_end = dates[0], dates[-period - 2]\n",
    "    holdout_start, holdout_end = dates[-period - 1], dates[-1]\n",
    "\n",
    "    df = features.join(target.to_frame())\n",
    "    train = df.loc[idx[:, cv_start: cv_end], :]\n",
    "    y_train, X_train = train[label], train.drop(label, axis=1)\n",
    "\n",
    "    test = df.loc[idx[:, holdout_start: holdout_end], :]\n",
    "    y_test, X_test = test[label], test.drop(label, axis=1)\n",
    "    return y_train, X_train, y_test, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79537f18-0f45-4afb-baa0-2fda00391fbe",
   "metadata": {},
   "source": [
    "### Custom TimeSeriesSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "254f2a18-1d42-4a99-b001-5553bb7780f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneStepTimeSeriesSplit:\n",
    "    \"\"\"Generates tuples of train_idx, test_idx pairs\n",
    "    Assumes the index contains a level labeled 'date'\"\"\"\n",
    "\n",
    "    def __init__(self, n_splits=3, test_period_length=1, shuffle=False):\n",
    "        self.n_splits = n_splits\n",
    "        self.test_period_length = test_period_length\n",
    "        self.shuffle = shuffle\n",
    "        self.test_end = n_splits * test_period_length\n",
    "\n",
    "    @staticmethod\n",
    "    def chunks(l, n):\n",
    "        for i in range(0, len(l), n):\n",
    "            yield l[i:i + n]\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        unique_dates = (X\n",
    "                            .index\n",
    "                            .get_level_values('date')\n",
    "                            .unique()\n",
    "                            .sort_values(ascending=False)\n",
    "        [:self.test_end])\n",
    "\n",
    "        dates = X.reset_index()[['date']]\n",
    "        for test_date in self.chunks(unique_dates, self.test_period_length):\n",
    "            train_idx = dates[dates.date < min(test_date)].index\n",
    "            test_idx = dates[dates.date.isin(test_date)].index\n",
    "            if self.shuffle:\n",
    "                np.random.shuffle(list(train_idx))\n",
    "            yield train_idx, test_idx\n",
    "    \n",
    "    def get_n_splits(self, X, y, groups=None):\n",
    "        return self.n_splits   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79edfdcd-5b9c-4642-89bb-f0731cc8eeb9",
   "metadata": {},
   "source": [
    "### Instantiating `GradientBoostingClassifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a8dfba6-1c77-4906-a3a6-0c7bfc10dcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_clf = GradientBoostingClassifier(loss='deviance',\n",
    "                                    learning_rate=0.1,\n",
    "                                    n_estimators=100,\n",
    "                                    subsample=1.0,\n",
    "                                    criterion='friedman_mse',\n",
    "                                    min_samples_split=2,\n",
    "                                    min_samples_leaf=1,\n",
    "                                    min_weight_fraction_leaf=0.0,\n",
    "                                    max_depth=3,\n",
    "                                    min_impurity_decrease=0.0,\n",
    "                                    # min_impurity_split=None,\n",
    "                                    init=None,\n",
    "                                    random_state=None,\n",
    "                                    max_features=None,\n",
    "                                    verbose=0,\n",
    "                                    max_leaf_nodes=None,\n",
    "                                    warm_start=False,\n",
    "                                    # presort='auto',\n",
    "                                    validation_fraction=0.1,\n",
    "                                    n_iter_no_change=None,\n",
    "                                    tol=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549899c0-b994-4e07-a2a6-ad4bb40b5a5d",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "592e8b0c-556c-414e-ad3d-152fbcac84dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_STORE = Path('../data/assets.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4cbaf0a-6958-429e-8d6e-ec51c591a7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(start='2010', end='2018', holding_period=1, dropna=False):\n",
    "    idx = pd.IndexSlice\n",
    "    target = f'target_{holding_period}m'\n",
    "    with pd.HDFStore(DATA_STORE) as store:\n",
    "        df = store['engineered_features']\n",
    "\n",
    "    if start is not None and end is not None:\n",
    "        df = df.loc[idx[:, start: end], :]\n",
    "    if dropna:\n",
    "        df = df.dropna()\n",
    "\n",
    "    y = (df[target] > 0).astype(int)\n",
    "    X = df.drop([c for c in df.columns if c.startswith('target')], axis=1)\n",
    "    return y, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9e81540-919a-401b-98f0-77725e4844c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 12\n",
    "\n",
    "cv = OneStepTimeSeriesSplit(n_splits=n_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7152f64-a272-4809-8c46-4122ee5dda18",
   "metadata": {},
   "outputs": [],
   "source": [
    "y, features = get_data()\n",
    "X = get_one_hot_data(features).dropna()\n",
    "\n",
    "y, X, y_test, X_test = get_holdout_set(target=y,\n",
    "                                       features=X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d849d7b-6ea5-439e-99de-6f944050c09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path('data')\n",
    "\n",
    "if not data_path.exists():\n",
    "    data_path.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "139790a6-8008-48c9-a6e5-ca7b70eceffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.HDFStore(data_path / 'tuning_sklearn_gbm.h5') as store:\n",
    "    store.put('holdout/features', X_test)\n",
    "    store.put('holdout/target', y_test)\n",
    "    store.put('cv/target', y)\n",
    "    store.put('cv/features', X)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a66f19-87cd-49ef-a55a-53e642ccb773",
   "metadata": {},
   "source": [
    "### `GridSearchCV` Setup\n",
    "\n",
    "#### Parameter Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76fabe9b-b431-4efd-bce7-8fd51891a5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = dict(\n",
    "        learning_rate=[.01, .1, .2],\n",
    "        max_depth=list(range(3, 13, 3)),\n",
    "        max_features=['sqrt', .8, 1],\n",
    "        min_impurity_decrease=[0, .01],\n",
    "        min_samples_split=[10, 50],\n",
    "        n_estimators=[100, 300],\n",
    "        subsample=[.8, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a25daf6d-3381-430d-a784-892574e3c8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Models = : 576\n"
     ]
    }
   ],
   "source": [
    "all_params = list(product(*param_grid.values()))\n",
    "\n",
    "print('# Models = :', len(all_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b72df56-1d46-4467-8649-a3f344dc7f45",
   "metadata": {},
   "source": [
    "#### Instantiating `GridSearchCV`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02891057-368e-4c7c-88f0-f76cc741f63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = GridSearchCV(gb_clf,\n",
    "                  param_grid,\n",
    "                  cv=cv,\n",
    "                  scoring='roc_auc',\n",
    "                  verbose=3,\n",
    "                  n_jobs=-1,\n",
    "                  return_train_score=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f35eebd-6d5b-4e2d-b5bc-bb832de3d0d7",
   "metadata": {},
   "source": [
    "#### Fitting `GridSearchCV`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58677f61-fbad-4636-9e34-b6b8f993584b",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "\n",
    "gs.fit(X=X, y=y)\n",
    "\n",
    "done = time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a13f61-93d0-4b8a-8aaf-d0df4a79703c",
   "metadata": {},
   "source": [
    "#### Persisting Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d5676975-8323-42ae-9b98-49ef460d680b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Done in {done:.2f}s')\n",
    "\n",
    "joblib.dump(gs, 'results/sklearn_gbm_gridsearch.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74751dae-370b-48c6-90be-9c1bd2afb595",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
