{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "073926db-f5cd-4c2e-889c-555f8be54950",
   "metadata": {},
   "source": [
    "# Word Vectors from SEC Filings using `Gensim`: Preprocessing\n",
    "\n",
    "### Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f959356-d6b8-417b-9a24-dd5fd70706da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical Computing\n",
    "import numpy as np\n",
    "\n",
    "# Data Manipulation\n",
    "import pandas as pd\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "\n",
    "# Data Visualizaion\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Path. Time & Collection\n",
    "import logging\n",
    "from time import time\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "# Spacy\n",
    "# import spacy\n",
    "\n",
    "# Gensim\n",
    "# from gensim.models import Word2Vec\n",
    "# from gensim.models.word2vec import LineSentence\n",
    "# from gensim.models.phrases import Phrases, Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "153ce37f-414a-41db-b03f-1619444e298f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9e9a500-034e-4d97-9f8d-52aaa0541ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(t):\n",
    "    m, s = divmod(t, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    return f'{h:02.0f}:{m:02.0f}:{s:02.0f}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddffb3a-0232-45c4-be04-50fb82c1a710",
   "metadata": {},
   "source": [
    "#### Logging Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7acd71e-0e44-4532-96a6-6dc09ba29515",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "        filename='preprocessing.log',\n",
    "        level=logging.DEBUG,\n",
    "        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "        datefmt='%H:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef78f22-1540-472f-a64f-a857e40df837",
   "metadata": {},
   "source": [
    "### Data Download\n",
    "\n",
    "#### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12e7d184-7603-4047-83ae-537969fea19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sec_path = Path('..', 'data', 'sec-filings')\n",
    "\n",
    "filing_path = sec_path / 'filings'\n",
    "\n",
    "sections_path = sec_path / 'sections'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8403b709-8715-4455-994e-fed89f2990b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not sections_path.exists():\n",
    "    sections_path.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88f5a92-8ab8-4249-b6d8-3d2fc77c5368",
   "metadata": {},
   "source": [
    "### Identify Sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bf96340-33c2-400f-b65c-8095586b4795",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, filing in enumerate(filing_path.glob('*.txt'), 1):\n",
    "    if i % 500 == 0:\n",
    "        print(i, end=' ', flush=True)\n",
    "    filing_id = int(filing.stem)\n",
    "    items = {}\n",
    "    for section in filing.read_text().lower().split('Â°'):\n",
    "        if section.startswith('item '):\n",
    "            if len(section.split()) > 1:\n",
    "                item = section.split()[1].replace('.', '').replace(':', '').replace(',', '')\n",
    "                text = ' '.join([t for t in section.split()[2:]])\n",
    "                if items.get(item) is None or len(items.get(item)) < len(text):\n",
    "                    items[item] = text\n",
    "\n",
    "    txt = pd.Series(items).reset_index()\n",
    "    txt.columns = ['item', 'text']\n",
    "    txt.to_csv(sections_path / (filing.stem + '.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32b1f46-72f6-4340-adc0-70b15c8d3c76",
   "metadata": {},
   "source": [
    "### Parsing Sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3aa2447-e6a6-41a4-9529-3d6d167735c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sections = ['1', '1a', '7', '7a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e682df0a-4c26-4274-9024-f54ad94031c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_path = sec_path / 'selected_sections'\n",
    "\n",
    "if not clean_path.exists():\n",
    "    clean_path.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9579c952-6a75-48a9-9f73-d7f05d46238e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm', disable=['ner'])\n",
    "\n",
    "nlp.max_length = 6000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8afcb3f9-67de-4f8f-9411-9637b1c9d41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Counter()\n",
    "\n",
    "t = total_tokens = 0\n",
    "\n",
    "stats = []\n",
    "\n",
    "start = time()\n",
    "\n",
    "to_do = len(list(sections_path.glob('*.csv')))\n",
    "\n",
    "done = len(list(clean_path.glob('*.csv'))) + 1\n",
    "\n",
    "for text_file in sections_path.glob('*.csv'):\n",
    "    file_id = int(text_file.stem)\n",
    "    clean_file = clean_path / f'{file_id}.csv'\n",
    "    if clean_file.exists():\n",
    "        continue\n",
    "    items = pd.read_csv(text_file).dropna()\n",
    "    items.item = items.item.astype(str)\n",
    "    items = items[items.item.isin(sections)]\n",
    "    if done % 100 == 0:\n",
    "        duration = time() - start\n",
    "        to_go = (to_do - done) * duration / done\n",
    "        print(f'{done:>5}\\t{format_time(duration)}\\t{total_tokens / duration:,.0f}\\t{format_time(to_go)}')\n",
    "    \n",
    "    clean_doc = []\n",
    "    for _, (item, text) in items.iterrows():\n",
    "        doc = nlp(text)\n",
    "        for s, sentence in enumerate(doc.sents):\n",
    "            clean_sentence = []\n",
    "            if sentence is not None:\n",
    "                for t, token in enumerate(sentence, 1):\n",
    "                    if not any([token.is_stop,\n",
    "                                token.is_digit,\n",
    "                                not token.is_alpha,\n",
    "                                token.is_punct,\n",
    "                                token.is_space,\n",
    "                                token.lemma_ == '-PRON-',\n",
    "                                token.pos_ in ['PUNCT', 'SYM', 'X']]):\n",
    "                        clean_sentence.append(token.text.lower())\n",
    "                total_tokens += t\n",
    "                if len(clean_sentence) > 0:\n",
    "                    clean_doc.append([item, s, ' '.join(clean_sentence)])\n",
    "    (pd.DataFrame(clean_doc,\n",
    "                  columns=['item', 'sentence', 'text'])\n",
    "     .dropna()\n",
    "     .to_csv(clean_file, index=False))\n",
    "    done += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7cc666-c14d-43f4-9039-b1972980886f",
   "metadata": {},
   "source": [
    "### Creating `ngrams`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "64337f30-cc74-4c2b-8283-5b558330532a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_path = sec_path / 'ngrams'\n",
    "\n",
    "stats_path = sec_path / 'corpus_stats'\n",
    "\n",
    "for path in [ngram_path, stats_path]:\n",
    "    if not path.exists():\n",
    "        path.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2117ad00-e51a-4e3e-971c-1a8e0669bb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams = ngram_path / 'ngrams_1.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "10641f9a-95e5-407b-8ca8-439cc113958d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_unigrams(min_length=3):\n",
    "    texts = []\n",
    "    sentence_counter = Counter()\n",
    "    vocab = Counter()\n",
    "    for i, f in enumerate(clean_path.glob('*.csv')):\n",
    "        if i % 1000 == 0:\n",
    "            print(i, end=' ', flush=True)\n",
    "        df = pd.read_csv(f)\n",
    "        df.item = df.item.astype(str)\n",
    "        df = df[df.item.isin(sections)]\n",
    "        sentence_counter.update(df.groupby('item').size().to_dict())\n",
    "        for sentence in df.text.dropna().str.split().tolist():\n",
    "            if len(sentence) >= min_length:\n",
    "                vocab.update(sentence)\n",
    "                texts.append(' '.join(sentence))\n",
    "    \n",
    "    (pd.DataFrame(sentence_counter.most_common(), \n",
    "                  columns=['item', 'sentences'])\n",
    "     .to_csv(stats_path / 'selected_sentences.csv', index=False))\n",
    "    (pd.DataFrame(vocab.most_common(), columns=['token', 'n'])\n",
    "     .to_csv(stats_path / 'sections_vocab.csv', index=False))\n",
    "    \n",
    "    unigrams.write_text('\\n'.join(texts))\n",
    "    return [l.split() for l in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "49240e57-6e84-43d3-a671-8315394aeac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading:  00:00:00\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "\n",
    "if not unigrams.exists():\n",
    "    texts = create_unigrams()\n",
    "else:\n",
    "    texts = [l.split() for l in unigrams.open()]\n",
    "print('\\nReading: ', format_time(time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b3be261e-db6c-4139-899a-b73bd3f65510",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ngrams(max_length=3):\n",
    "    \"\"\"Using gensim to create ngrams\"\"\"\n",
    "\n",
    "    n_grams = pd.DataFrame()\n",
    "    start = time()\n",
    "    for n in range(2, max_length + 1):\n",
    "        print(n, end=' ', flush=True)\n",
    "\n",
    "        sentences = LineSentence(ngram_path / f'ngrams_{n - 1}.txt')\n",
    "        phrases = Phrases(sentences=sentences,\n",
    "                          min_count=25,  # ignore terms with a lower count\n",
    "                          threshold=0.5,  # accept phrases with higher score\n",
    "                          max_vocab_size=40000000,  # prune of less common words to limit memory use\n",
    "                          delimiter=b'_',  # how to join ngram tokens\n",
    "                          progress_per=50000,  # log progress every\n",
    "                          scoring='npmi')\n",
    "\n",
    "        s = pd.DataFrame([[k.decode('utf-8'), v] for k, v in phrases.export_phrases(sentences)], \n",
    "                         columns=['phrase', 'score']).assign(length=n)\n",
    "\n",
    "        n_grams = pd.concat([n_grams, s])\n",
    "        grams = Phraser(phrases)\n",
    "        sentences = grams[sentences]\n",
    "        (ngram_path / f'ngrams_{n}.txt').write_text('\\n'.join([' '.join(s) for s in sentences]))\n",
    "\n",
    "    n_grams = n_grams.sort_values('score', ascending=False)\n",
    "    n_grams.phrase = n_grams.phrase.str.replace('_', ' ')\n",
    "    n_grams['ngram'] = n_grams.phrase.str.replace(' ', '_')\n",
    "\n",
    "    n_grams.to_parquet(sec_path / 'ngrams.parquet')\n",
    "\n",
    "    print('\\n\\tDuration: ', format_time(time() - start))\n",
    "    print('\\tngrams: {:,d}\\n'.format(len(n_grams)))\n",
    "    print(n_grams.groupby('length').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b83127d-9320-4d5a-a19f-669329b10a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_ngrams()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28d8022-1c65-4217-aa2f-eec8a41a59ac",
   "metadata": {},
   "source": [
    "### Inspecting Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1b5b51fb-937f-422a-982d-7005d281aaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentiles=np.arange(.1, 1, .1).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "024a9045-498e-4ed8-b0c1-b006a76d5546",
   "metadata": {},
   "outputs": [],
   "source": [
    "nsents, ntokens = Counter(), Counter()\n",
    "\n",
    "for f in clean_path.glob('*.csv'):\n",
    "    df = pd.read_csv(f)\n",
    "    nsents.update({str(k): v for k, v in df.item.value_counts().to_dict().items()})\n",
    "    df['ntokens'] = df.text.str.split().str.len()\n",
    "    ntokens.update({str(k): v for k, v in df.groupby('item').ntokens.sum().to_dict().items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9d14fdc9-894b-4aa1-bd5f-9aed41b495c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = pd.DataFrame(ntokens.most_common(), columns=['Item', '# Tokens'])\n",
    "\n",
    "nsents = pd.DataFrame(nsents.most_common(), columns=['Item', '# Sentences'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "47cc4d6e-7df7-43f9-9f7b-470355df9d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "nsents.set_index('Item').join(ntokens.set_index('Item')).plot.bar(secondary_y='# Tokens', rot=0);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ff3a519e-a095-47d0-ae1b-8d393817f398",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams = pd.read_parquet(sec_path / 'ngrams.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "221d885b-d39a-466b-bf86-ea4596d8e300",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e0ebf1e5-b2a9-40c1-9881-8a466b1c057b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "44d2b091-9d3e-405f-931a-495206d2d412",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams.score.describe(percentiles=percentiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4a056b5c-7461-4ac7-9966-f88ad966a8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams[ngrams.score>.7].sort_values(['length', 'score']).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5f1a4ea8-5989-4399-aae1-ae38c28e7937",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = pd.read_csv(stats_path / 'sections_vocab.csv').dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fd39578c-3e11-4869-949a-ba092a4ae890",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "75da797f-684c-48a3-9604-02868911bd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.n.describe(percentiles).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ade478ea-98ef-4a6f-b330-125ff728a6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = Counter()\n",
    "\n",
    "for l in (ngram_path / 'ngrams_2.txt').open():\n",
    "    tokens.update(l.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "252ed45a-22f9-4b0a-a0b2-8360b0319b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = pd.DataFrame(tokens.most_common(),\n",
    "                     columns=['token', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "12b5b81c-90fb-49c3-af2b-b18bbeea31df",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6b46bda7-c0fd-4d7b-8c59-d654ead40022",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6f6f85cf-3050-4447-ac09-bb19f3540b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens.loc[tokens.token.str.contains('_'), 'count'].describe(percentiles).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "440d7174-2d15-48d3-9ce2-03b0ff66837a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens[tokens.token.str.contains('_')].head(20).to_csv(sec_path / 'ngram_examples.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e636228e-6618-4620-b570-0fffa8b6575f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens[tokens.token.str.contains('_')].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e805de3-30a8-4c86-87c5-b6ac2989b7ec",
   "metadata": {},
   "source": [
    "### Getting Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9dde444d-56e8-4d9a-9438-e96c81aa7773",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = Path('..', 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e5efd795-1cb9-444f-9546-58fbf05d9db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.HDFStore(DATA_FOLDER / 'assets.h5') as store:\n",
    "    prices = store['quandl/wiki/prices'].adj_close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fa2ded6a-eb16-4402-a962-3b259d78ea96",
   "metadata": {},
   "outputs": [],
   "source": [
    "sec = pd.read_csv(sec_path / 'filing_index.csv').rename(columns=str.lower)\n",
    "\n",
    "sec.date_filed = pd.to_datetime(sec.date_filed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "85a15e6a-d04b-4f68-8b90-bfbb0e0edfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sec.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a5f328c7-20da-4bc0-9cd4-b9a2abe05c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = pd.IndexSlice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "56c2a9fa-8412-45a4-87b4-f2d027a61f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "first = sec.date_filed.min() + relativedelta(months=-1)\n",
    "\n",
    "last = sec.date_filed.max() + relativedelta(months=1)\n",
    "\n",
    "prices = (prices\n",
    "          .loc[idx[first:last, :]]\n",
    "          .unstack().resample('D')\n",
    "          .ffill()\n",
    "          .dropna(how='all', axis=1)\n",
    "          .filter(sec.ticker.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ca49d336-97f2-4fe9-9dc7-ae6430cd23a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sec = sec.loc[sec.ticker.isin(prices.columns), ['ticker', 'date_filed']]\n",
    "\n",
    "price_data = []\n",
    "for ticker, date in sec.values.tolist():\n",
    "    target = date + relativedelta(months=1)\n",
    "    s = prices.loc[date: target, ticker]\n",
    "    price_data.append(s.iloc[-1] / s.iloc[0] - 1)\n",
    "\n",
    "df = pd.DataFrame(price_data,\n",
    "                  columns=['returns'],\n",
    "                  index=sec.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "01e682d8-c9e7-4eb1-81b4-af76c1ef058b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.returns.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "268fa1bd-399f-4233-8d90-9c34c26b2ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sec['returns'] = price_data\n",
    "sec.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6346bf59-cbb7-440a-a13f-8f5d62d73ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sec.dropna().to_csv(sec_path / 'sec_returns.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53eced2-bece-4cd6-8d0b-be2b25a53246",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
